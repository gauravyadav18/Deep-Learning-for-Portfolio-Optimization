{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0200941b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, LSTM\n",
    "from tensorflow.keras.models import Model\n",
    "import cvxpy as cp\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from datetime import datetime, timedelta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1340bace",
   "metadata": {},
   "source": [
    "### Set random seeds\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb20870e",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dae7204",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_periods(train_start, train_end, val_length_years, test_length_years, data_min, data_max):\n",
    "    train_start = pd.to_datetime(train_start)\n",
    "    train_end = pd.to_datetime(train_end)\n",
    "    data_min = pd.to_datetime(data_min)\n",
    "    data_max = pd.to_datetime(data_max)\n",
    "    \n",
    "    periods = {'train': [], 'val': [], 'test': []}\n",
    "    \n",
    "    # Adjust train_start and train_end if outside data range\n",
    "    if train_start < data_min:\n",
    "        print(f\"Adjusting train_start from {train_start} to {data_min}\")\n",
    "        train_start = data_min\n",
    "    if train_end > data_max:\n",
    "        print(f\"Adjusting train_end from {train_end} to {data_max}\")\n",
    "        train_end = data_max\n",
    "    \n",
    "    # Set validation and test periods\n",
    "    val_start = train_end + timedelta(days=1)\n",
    "    val_end = val_start + timedelta(days=365 * val_length_years - 1)\n",
    "    test_start = val_end + timedelta(days=1)\n",
    "    test_end = test_start + timedelta(days=365 * test_length_years - 1)\n",
    "    \n",
    "    # Adjust if test_end exceeds data_max\n",
    "    if test_end > data_max:\n",
    "        print(f\"Adjusting test_end from {test_end} to {data_max}\")\n",
    "        test_end = data_max\n",
    "    \n",
    "    # Validate periods\n",
    "    if val_start > val_end or test_start > test_end or train_end >= val_start or val_end >= test_start:\n",
    "        print(\"Error: Invalid or overlapping periods.\")\n",
    "        return [], [], []\n",
    "    \n",
    "    periods['train'].append((train_start.strftime('%Y-%m-%d'), train_end.strftime('%Y-%m-%d')))\n",
    "    periods['val'].append((val_start.strftime('%Y-%m-%d'), val_end.strftime('%Y-%m-%d')))\n",
    "    periods['test'].append((test_start.strftime('%Y-%m-%d'), test_end.strftime('%Y-%m-%d')))\n",
    "    \n",
    "    if not periods['train']:\n",
    "        print(\"Warning: No valid periods generated.\")\n",
    "    \n",
    "    return periods['train'], periods['val'], periods['test']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ffee2f1",
   "metadata": {},
   "source": [
    "### Load datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2071a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    index1 = pd.read_csv('AGG.csv', parse_dates=['Date'])\n",
    "    index2 = pd.read_csv('VTI.csv', parse_dates=['Date'])\n",
    "    index3 = pd.read_csv('VIX.csv', parse_dates=['Date'])\n",
    "    index4 = pd.read_csv('DBC.csv', parse_dates=['Date'])\n",
    "except FileNotFoundError as e:\n",
    "    print(f\"Error: Dataset file not found. Ensure index1.csv, index2.csv, index3.csv, index4.csv exist. {e}\")\n",
    "    raise\n",
    "\n",
    "# Select columns and compute features\n",
    "index1 = index1[['Date', 'Close', 'Open', 'High', 'Low']].set_index('Date')\n",
    "index1['Volatility'] = index1['High'] - index1['Low']\n",
    "index1['Momentum'] = index1['Close'] - index1['Open']\n",
    "index1 = index1[['Close', 'Volatility', 'Momentum']].rename(columns={'Close': 'Index1_Close', 'Volatility': 'Index1_Volatility', 'Momentum': 'Index1_Momentum'})\n",
    "\n",
    "index2 = index2[['Date', 'Close', 'Open', 'High', 'Low']].set_index('Date')\n",
    "index2['Volatility'] = index2['High'] - index2['Low']\n",
    "index2['Momentum'] = index2['Close'] - index2['Open']\n",
    "index2 = index2[['Close', 'Volatility', 'Momentum']].rename(columns={'Close': 'Index2_Close', 'Volatility': 'Index2_Volatility', 'Momentum': 'Index2_Momentum'})\n",
    "\n",
    "index3 = index3[['Date', 'Close', 'Open', 'High', 'Low']].set_index('Date')\n",
    "index3['Volatility'] = index3['High'] - index3['Low']\n",
    "index3['Momentum'] = index3['Close'] - index3['Open']\n",
    "index3 = index3[['Close', 'Volatility', 'Momentum']].rename(columns={'Close': 'Index3_Close', 'Volatility': 'Index3_Volatility', 'Momentum': 'Index3_Momentum'})\n",
    "\n",
    "index4 = index4[['Date', 'Close', 'Open', 'High', 'Low']].set_index('Date')\n",
    "index4['Volatility'] = index4['High'] - index4['Low']\n",
    "index4['Momentum'] = index4['Close'] - index4['Open']\n",
    "index4 = index4[['Close', 'Volatility', 'Momentum']].rename(columns={'Close': 'Index4_Close', 'Volatility': 'Index4_Volatility', 'Momentum': 'Index4_Momentum'})\n",
    "\n",
    "# Create daily date range\n",
    "all_dates = pd.date_range(\n",
    "    start=min(index1.index.min(), index2.index.min(), index3.index.min(), index4.index.min()),\n",
    "    end=max(index1.index.max(), index2.index.max(), index3.index.max(), index4.index.max()),\n",
    "    freq='D'\n",
    ")\n",
    "\n",
    "# Reindex and forward-fill\n",
    "index1 = index1.reindex(all_dates).fillna(method='ffill')\n",
    "index2 = index2.reindex(all_dates).fillna(method='ffill')\n",
    "index3 = index3.reindex(all_dates).fillna(method='ffill')\n",
    "index4 = index4.reindex(all_dates).fillna(method='ffill')\n",
    "\n",
    "# Merge\n",
    "data = index1.join([index2, index3, index4], how='outer').sort_index()\n",
    "print(f\"Data range: {data.index.min()} to {data.index.max()}\")\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "\n",
    "# Handle missing data\n",
    "data = data.fillna(method='ffill').dropna()\n",
    "if data.empty:\n",
    "    print(\"Error: Merged data is empty after handling missing values.\")\n",
    "    raise ValueError(\"Empty dataset\")\n",
    "\n",
    "# Calculate logarithmic returns (for Close only)\n",
    "close_data = data[['Index1_Close', 'Index2_Close', 'Index3_Close', 'Index4_Close']].rename(\n",
    "    columns={'Index1_Close': 'Index1', 'Index2_Close': 'Index2', 'Index3_Close': 'Index3', 'Index4_Close': 'Index4'}\n",
    ")\n",
    "returns = np.log(close_data.pct_change() + 1).dropna()\n",
    "if returns.empty:\n",
    "    print(\"Error: Returns data is empty.\")\n",
    "    raise ValueError(\"Empty returns\")\n",
    "\n",
    "# Risk-free rate (least volatile index)\n",
    "volatilities = returns.std() * np.sqrt(252)\n",
    "min_vol_index = volatilities.idxmin()\n",
    "risk_free_rate = returns[min_vol_index].mean() * 252\n",
    "risk_free_rate_daily = risk_free_rate / 252\n",
    "print(f\"Risk-Free Rate (from {min_vol_index}): {risk_free_rate:.4f} (annualized)\")\n",
    "\n",
    "# Alternative: Fixed 1% risk-free rate\n",
    "fixed_risk_free_rate = 0.01\n",
    "fixed_risk_free_rate_daily = fixed_risk_free_rate / 252\n",
    "\n",
    "# Define indices\n",
    "indices = close_data.columns\n",
    "\n",
    "## Set Custom Periods\n",
    "# Training 2006–2017, validation 2018, test 2019–2025.\n",
    "\n",
    "train_start = '2006-01-01'  # Adjusts to 2006-02-06\n",
    "train_end = '2017-12-31'\n",
    "val_length_years = 1  # 2018\n",
    "test_length_years = 2 # 2019–2025\n",
    "\n",
    "train_periods, val_periods, test_periods = generate_periods( train_start, train_end, val_length_years, test_length_years,data_min=data.index.min(), data_max=data.index.max()\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "dc43ed4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def create_windows(features, returns=None, window_size=90):\n",
    "    if len(features) < window_size + 1:\n",
    "        return np.array([]), np.array([])\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - window_size):\n",
    "        window = features[i:i+window_size]\n",
    "        if len(window) == window_size:\n",
    "            X.append(window)\n",
    "            # Use returns for y if provided (e.g., test_returns_idx), else use features[:, 0]\n",
    "            if returns is not None:\n",
    "                y.append(returns[i+window_size])\n",
    "            else:\n",
    "                y.append(features[i+window_size, 0])  # Predict Close\n",
    "    return np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fdb0507",
   "metadata": {},
   "source": [
    "### Autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25f6bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define AE with input for 90 days * 3 features.\n",
    "\n",
    "def build_autoencoder(input_dim=90*3, encoding_dim=10):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    encoded = Dense(50, activation='relu')(input_layer)\n",
    "    encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "    decoded = Dense(50, activation='relu')(encoded)\n",
    "    decoded = Dense(input_dim, activation='linear')(decoded)\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return autoencoder, encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6b0fcd9",
   "metadata": {},
   "source": [
    "\n",
    "### LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2b1c92",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define LSTM with 20 nodes.\n",
    "\n",
    "def build_lstm(encoding_dim=10):\n",
    "    input_layer = Input(shape=(1, encoding_dim))\n",
    "    lstm = LSTM(20, dropout=0.1, recurrent_dropout=0.2)(input_layer)\n",
    "    output = Dense(1)(lstm)\n",
    "    model = Model(input_layer, output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.SGD(learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f694909",
   "metadata": {},
   "source": [
    "### MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610d3986",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define MLP.\n",
    "\n",
    "def build_mlp(encoding_dim=10):\n",
    "    input_layer = Input(shape=(encoding_dim,))\n",
    "    dense = Dense(30, activation='relu')(input_layer)\n",
    "    dense = Dense(15, activation='relu')(dense)\n",
    "    output = Dense(1)(dense)\n",
    "    model = Model(input_layer, output)\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b57c09fd",
   "metadata": {},
   "source": [
    "### Omega Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db689652",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define OMEGA with δ=0.7, weight cap, volatility penalty.\n",
    "\n",
    "def optimize_omega(predicted_returns, actual_returns, R_20, R_5, cov_matrix, delta=0.7):\n",
    "    n = len(indices)\n",
    "    x = cp.Variable(n)\n",
    "    psi = cp.Variable()\n",
    "    u = cp.Variable(10)  # T^j = 10\n",
    "    l = 2  # Number of normal distributions\n",
    "    \n",
    "    e_hat = np.zeros((n, 10))  # Predictive errors placeholder\n",
    "    \n",
    "    # Volatility penalty\n",
    "    portfolio_variance = cp.quad_form(x, cov_matrix)\n",
    "    volatility_penalty = 0.1 * cp.sqrt(portfolio_variance)\n",
    "    \n",
    "    obj = (2/8) * psi + (2/8) * cp.sum(x @ predicted_returns) + (2/8) * cp.sum(x @ np.mean(e_hat, axis=1)) + \\\n",
    "          (1/8) * cp.sum(x @ R_20) + (1/8) * cp.sum(x @ R_5) - volatility_penalty\n",
    "    \n",
    "    constraints = []\n",
    "    for j in range(l):\n",
    "        constraints.append(delta * cp.sum(x @ predicted_returns) - ((1 - delta) / 10) * cp.sum(u) >= psi)\n",
    "        for t in range(10):\n",
    "            constraints.append(u[t] >= -cp.sum(x @ e_hat[:, t]))\n",
    "            constraints.append(u[t] >= 0)\n",
    "    constraints.append(cp.sum(x) == 1)\n",
    "    constraints.append(x >= 0)\n",
    "    constraints.append(x <= 0.5)\n",
    "    \n",
    "    prob = cp.Problem(cp.Maximize(obj), constraints)\n",
    "    try:\n",
    "        prob.solve(solver=cp.ECOS)\n",
    "        if prob.status != 'optimal':\n",
    "            return np.ones(n) / n\n",
    "        return x.value\n",
    "    except:\n",
    "        return np.ones(n) / n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739446c2",
   "metadata": {},
   "source": [
    "### Equal-Weighted Portfolio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f58fadc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing period 0: Train 2006-02-06 00:00:00 to 2017-12-31 00:00:00, Val 2018-01-01 00:00:00 to 2018-12-31 00:00:00, Test 2019-01-01 00:00:00 to 2025-05-16 00:00:00\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Define AE+LSTM+EW.\n",
    "\n",
    "def equal_weighted_portfolio(n):\n",
    "    return np.ones(n) / n\n",
    "\n",
    "## Training and Testing\n",
    "# Train models and optimize portfolios with monthly rebalancing.\n",
    "\n",
    "all_portfolio_returns = {'AE+LSTM+OMEGA': [], 'AE+LSTM+EW': [], 'AE+MLP+OMEGA': []}\n",
    "if not train_periods:\n",
    "    print(\"Error: No training periods generated. Cannot proceed with training.\")\n",
    "else:\n",
    "    for i, (train_period, val_period, test_period) in enumerate(zip(train_periods, val_periods, test_periods)):\n",
    "        train_start, train_end = pd.to_datetime(train_period)\n",
    "        val_start, val_end = pd.to_datetime(val_period)\n",
    "        test_start, test_end = pd.to_datetime(test_period)\n",
    "        \n",
    "        print(f\"Processing period {i}: Train {train_start} to {train_end}, Val {val_start} to {val_end}, Test {test_start} to {test_end}\")\n",
    "        \n",
    "        try:\n",
    "            train_data = data.loc[train_start:train_end]\n",
    "            val_data = data.loc[val_start:val_end]\n",
    "            test_data = data.loc[test_start:test_end]\n",
    "            test_returns = returns.loc[test_start:test_end]\n",
    "            \n",
    "            if train_data.empty or val_data.empty or test_data.empty:\n",
    "                print(f\"Skipping period {i}: Empty data for train, val, or test.\")\n",
    "                continue\n",
    "        except KeyError as e:\n",
    "            print(f\"Skipping period {i}: Date range error. {e}\")\n",
    "            continue\n",
    "        \n",
    "        cov_matrix = test_returns.cov() * 252\n",
    "        \n",
    "        predicted_returns_lstm = {}\n",
    "        predicted_returns_mlp = {}\n",
    "        for idx in indices:\n",
    "            train_features = train_data[[f'{idx}_Close', f'{idx}_Volatility', f'{idx}_Momentum']].values\n",
    "            val_features = val_data[[f'{idx}_Close', f'{idx}_Volatility', f'{idx}_Momentum']].values\n",
    "            test_features = test_data[[f'{idx}_Close', f'{idx}_Volatility', f'{idx}_Momentum']].values\n",
    "            train_returns_idx = returns[idx].loc[train_start:train_end].values\n",
    "            val_returns_idx = returns[idx].loc[val_start:val_end].values\n",
    "            test_returns_idx = returns[idx].loc[test_start:test_end].values\n",
    "            \n",
    "            # Use features for X, returns for y in test\n",
    "            X_train, y_train = create_windows(train_features)\n",
    "            X_val, y_val = create_windows(val_features)\n",
    "            X_test, y_test = create_windows(test_features, returns=test_returns_idx)\n",
    "            \n",
    "            if len(X_train) == 0 or len(X_val) == 0 or len(X_test) == 0:\n",
    "                print(f\"Skipping index {idx}: Insufficient data for windows.\")\n",
    "                continue\n",
    "            \n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train.reshape(X_train.shape[0], -1))\n",
    "            X_val_scaled = scaler.transform(X_val.reshape(X_val.shape[0], -1))\n",
    "            X_test_scaled = scaler.transform(X_test.reshape(X_test.shape[0], -1))\n",
    "            \n",
    "            autoencoder, encoder = build_autoencoder()\n",
    "            autoencoder.fit(X_train_scaled, X_train_scaled, epochs=50, batch_size=100, validation_data=(X_val_scaled, X_val_scaled), verbose=0)\n",
    "            \n",
    "            X_train_encoded = encoder.predict(X_train_scaled, verbose=0)\n",
    "            X_val_encoded = encoder.predict(X_val_scaled, verbose=0)\n",
    "            X_test_encoded = encoder.predict(X_test_scaled, verbose=0)\n",
    "            \n",
    "            X_train_lstm = X_train_encoded.reshape(X_train_encoded.shape[0], 1, X_train_encoded.shape[1])\n",
    "            X_val_lstm = X_val_encoded.reshape(X_val_encoded.shape[0], 1, X_val_encoded.shape[1])\n",
    "            X_test_lstm = X_test_encoded.reshape(X_test_encoded.shape[0], 1, X_test_encoded.shape[1])\n",
    "            \n",
    "            lstm_model = build_lstm()\n",
    "            lstm_model.fit(X_train_lstm, y_train, epochs=50, batch_size=100, validation_data=(X_val_lstm, y_val), verbose=0)\n",
    "            predicted_lstm = lstm_model.predict(X_test_lstm, verbose=0).flatten()\n",
    "            predicted_returns_lstm[idx] = predicted_lstm\n",
    "            \n",
    "            mlp_model = build_mlp()\n",
    "            mlp_model.fit(X_train_encoded, y_train, epochs=50, batch_size=100, validation_data=(X_val_encoded, y_val), verbose=0)\n",
    "            predicted_mlp = mlp_model.predict(X_test_encoded, verbose=0).flatten()\n",
    "            predicted_returns_mlp[idx] = predicted_mlp\n",
    "        \n",
    "        if not predicted_returns_lstm or not predicted_returns_mlp:\n",
    "            print(f\"Skipping period {i}: No predictions generated.\")\n",
    "            continue\n",
    "        \n",
    "        R_20 = test_returns.rolling(window=20).mean().iloc[90:].values\n",
    "        R_5 = test_returns.rolling(window=5).mean().iloc[90:].values\n",
    "        \n",
    "        for t in range(0, len(predicted_returns_lstm[indices[0]]), 21):\n",
    "            actual_ret = test_returns.iloc[90+t].values\n",
    "            pred_ret_lstm = np.array([predicted_returns_lstm[idx][t] for idx in indices])\n",
    "            pred_ret_mlp = np.array([predicted_returns_mlp[idx][t] for idx in indices])\n",
    "            \n",
    "            weights_omega = optimize_omega(pred_ret_lstm, actual_ret, R_20[t], R_5[t], cov_matrix)\n",
    "            daily_return_omega = np.sum(actual_ret * weights_omega)\n",
    "            all_portfolio_returns['AE+LSTM+OMEGA'].append(daily_return_omega)\n",
    "            \n",
    "            weights_ew = equal_weighted_portfolio(len(indices))\n",
    "            daily_return_ew = np.sum(actual_ret * weights_ew)\n",
    "            all_portfolio_returns['AE+LSTM+EW'].append(daily_return_ew)\n",
    "            \n",
    "            weights_mlp = optimize_omega(pred_ret_mlp, actual_ret, R_20[t], R_5[t], cov_matrix)\n",
    "            daily_return_mlp = np.sum(actual_ret * weights_mlp)\n",
    "            all_portfolio_returns['AE+MLP+OMEGA'].append(daily_return_mlp)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd507a79",
   "metadata": {},
   "source": [
    "### Performance Metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48441e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "AE+LSTM+OMEGA:\n",
      "Sharpe Ratio (Min-Vol Index): 5.8100\n",
      "Sharpe Ratio (Fixed 1%): 5.8432\n",
      "Sharpe Ratio (Zero Risk-Free): 5.8750\n",
      "Annual Return: 1.8456\n",
      "Total Return: 1.1396\n",
      "\n",
      "AE+LSTM+EW:\n",
      "Sharpe Ratio (Min-Vol Index): 5.8100\n",
      "Sharpe Ratio (Fixed 1%): 5.8432\n",
      "Sharpe Ratio (Zero Risk-Free): 5.8750\n",
      "Annual Return: 1.8456\n",
      "Total Return: 1.1396\n",
      "\n",
      "AE+MLP+OMEGA:\n",
      "Sharpe Ratio (Min-Vol Index): 5.8100\n",
      "Sharpe Ratio (Fixed 1%): 5.8432\n",
      "Sharpe Ratio (Zero Risk-Free): 5.8750\n",
      "Annual Return: 1.8456\n",
      "Total Return: 1.1396\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate Sharpe ratios with both risk-free rates.\n",
    "\n",
    "results = {}\n",
    "for model in all_portfolio_returns:\n",
    "    portfolio_returns = np.array(all_portfolio_returns[model])\n",
    "    if len(portfolio_returns) == 0:\n",
    "        print(f\"Warning: No returns for {model}. Metrics set to zero.\")\n",
    "        results[model] = {\n",
    "            'Sharpe Ratio (Min-Vol)': 0.0,\n",
    "            'Sharpe Ratio (Fixed 1%)': 0.0,\n",
    "            'Sharpe Ratio (Zero)': 0.0,\n",
    "            'Annual Return': 0.0,\n",
    "            'Total Return': 0.0\n",
    "        }\n",
    "    else:\n",
    "        mean_return = np.mean(portfolio_returns) * 252\n",
    "        std_return = np.std(portfolio_returns) * np.sqrt(252)\n",
    "        sharpe_min_vol = (mean_return - risk_free_rate) / std_return if std_return != 0 else 0.0\n",
    "        sharpe_fixed = (mean_return - fixed_risk_free_rate) / std_return if std_return != 0 else 0.0\n",
    "        sharpe_zero = mean_return / std_return if std_return != 0 else 0.0\n",
    "        total_return = np.prod(1 + portfolio_returns) - 1\n",
    "        results[model] = {\n",
    "            'Sharpe Ratio (Min-Vol)': sharpe_min_vol,\n",
    "            'Sharpe Ratio (Fixed 1%)': sharpe_fixed,\n",
    "            'Sharpe Ratio (Zero)': sharpe_zero,\n",
    "            'Annual Return': mean_return,\n",
    "            'Total Return': total_return\n",
    "        }\n",
    "\n",
    "for model, metrics in results.items():\n",
    "    print(f\"\\n{model}:\")\n",
    "    print(f\"Sharpe Ratio (Min-Vol Index): {metrics['Sharpe Ratio (Min-Vol)']:.4f}\")\n",
    "    print(f\"Sharpe Ratio (Fixed 1%): {metrics['Sharpe Ratio (Fixed 1%)']:.4f}\")\n",
    "    print(f\"Sharpe Ratio (Zero Risk-Free): {metrics['Sharpe Ratio (Zero)']:.4f}\")\n",
    "    print(f\"Annual Return: {metrics['Annual Return']:.4f}\")\n",
    "    print(f\"Total Return: {metrics['Total Return']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
